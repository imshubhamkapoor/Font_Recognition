# Font_Recognition

## Introduction to Project

In this project, you'll train a convolutional neural network to classify and recognize different categories of fonts. We'll be using the dataset of 100 categories of fonts to train our model.
The project is broken down into multiple steps:
- Generating the dataset of fonts from package
- Loading and preprocessing the image dataset
- Visualization of samples from the dataset
- Train the Convolutional Neural Network on your dataset
- Use the trained model to predict new fonts

The whole project is implemented in tensorflow.

## Dataset Description

Adobe VFR dataset is the first large-scale, fine-grained benchmark of font text images, for the task of font recognition and retrieval. Unfortunately, it is very huge so I was unable to download it. Then, I discovered the [TRDG](https://textrecognitiondatagenerator.readthedocs.io/en/latest/index.html) (Text Recognition Data Generator) package created by Edouard Belval in Python. It is a synthetic data generator for text recognition.

### How does the Belval's original TRDG package work?

Words will be randomly chosen from a dictionary of a specific language. Then, an image of those words will be generated by using font, background, and modifications (skewing, blurring, etc.) as specified.
The usage as a Python module is very similar to the CLI.

But the problem arose when I had to create a dataset in a systematic manner with particular font images in their respective folders. As you can see that the original TRDG package generated random images in a single folder from the total list of fonts unless specified a particular font and directory. According to which, I have to run the package for each font and its directory again and again.

### How does my TRDG package work?

I modified the above package to suit my problem. I created a loop inside the main file (run.py) to create a directory for each font provided in the font list (folder) and store its images respectively. Now, after running the package once, it generates font images and stores them in their specified folder (named after that particular font) in a loop for each font provided in the font list. 

I have kept my package in the trdg folder to run it separately from the main pipeline. All the rest of the documentation and working is the same as per Belval's original package.

## Files Description

- **main_file.ipynb** It contains the full code and is used to build the model using the jupyter notebook. It can be used independently to see how the model works.
- **kmnist_classmap.csv** It is used in ipynb file to map from class IDs to unicode characters for Kuzushiji-MNIST.
- **kmnist-train-images.npz** It contains the training dataset of 60,000 images (28x28 grayscale) provided in a NumPy format.
- **kmnist-test-images.npz** It contains the test dataset of 10,000 images (28x28 grayscale) provided in a NumPy format.
- **kmnist-train-labels.npz** It contains the training labels of 60,000 images provided in a NumPy format for training dataset.
- **kmnist-test-labels.npz** It contains the test labels of 10,000 images provided in a NumPy format for test dataset.

**NOTE:** kmnist-[train/test]-[images/labels].npz: These files contain the Kuzushiji-MNIST as compressed numpy arrays, and can be read with: arr = np.load(filename)['arr_0']. We recommend using these files to load the dataset.

## Installation
The Code is written in Jupyter Notebook.

Additional Packages that are required are: Numpy, Pandas, MatplotLib, Pytorch, and PIL. You can donwload them using pip

`pip install numpy pandas matplotlib pil`

In order to intall Pytorch head over to the [Pytorch](https://pytorch.org/get-started/locally/) website and follow the instructions given.

## GPU/CPU

As this project uses deep CNNs, for training of network you need to use a GPU. However after training you can always use normal CPU for the prediction phase.

## License
[MIT License](https://github.com/imshubhamkapoor/Kuzushiji_MNIST_Japanese_Character_Classification/blob/master/LICENSE)

## Author
Shubham Kapoor
